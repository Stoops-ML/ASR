{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from train import main\n",
    "from processing import data_processing, TextTransform, char_map_str\n",
    "from networks import SpeechRecognitionModel\n",
    "import torch.utils.data as data\n",
    "import torchaudio\n",
    "from ctcdecode import CTCBeamDecoder\n",
    "from predict import predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    \"n_cnn_layers\": 3,\n",
    "    \"n_rnn_layers\": 5,\n",
    "    \"rnn_dim\": 512,\n",
    "    \"n_class\": 29,\n",
    "    \"n_feats\": 128,\n",
    "    \"stride\": 2,\n",
    "    \"dropout\": 0.1,\n",
    "    \"learning_rate\": 5e-4,\n",
    "    \"batch_size\": 20,\n",
    "    \"epochs\": 10\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transforms = nn.Sequential(\n",
    "    torchaudio.transforms.MelSpectrogram(sample_rate=16000, n_mels=128),\n",
    "    torchaudio.transforms.FrequencyMasking(freq_mask_param=30),\n",
    "    torchaudio.transforms.TimeMasking(time_mask_param=100)\n",
    ")\n",
    "test_transforms = torchaudio.transforms.MelSpectrogram()\n",
    "text_transforms = TextTransform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = torchaudio.datasets.LIBRISPEECH('data', url=\"train-clean-100\", download=True)\n",
    "test_dataset = torchaudio.datasets.LIBRISPEECH('data', url=\"test-clean\", download=True)\n",
    "\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if torch.cuda.is_available() else {}\n",
    "train_loader = data.DataLoader(dataset=train_dataset,\n",
    "                               batch_size=hyperparameters['batch_size'],\n",
    "                               shuffle=True,\n",
    "                               collate_fn=lambda x: data_processing(x, train_transforms, text_transforms),\n",
    "                               **kwargs)\n",
    "test_loader = data.DataLoader(dataset=test_dataset,\n",
    "                              batch_size=hyperparameters['batch_size'],\n",
    "                              shuffle=False,\n",
    "                              collate_fn=lambda x: data_processing(x, test_transforms, text_transforms),\n",
    "                              **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number model parameters 23705373\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SpeechRecognitionModel(\n",
       "  (cnn): Conv2d(1, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "  (rescnn_layers): Sequential(\n",
       "    (0): ResidualCNN(\n",
       "      (cnn1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (cnn2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (dropout1): Dropout(p=0.1, inplace=False)\n",
       "      (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      (layer_norm1): CNNLayerNorm(\n",
       "        (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (layer_norm2): CNNLayerNorm(\n",
       "        (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (1): ResidualCNN(\n",
       "      (cnn1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (cnn2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (dropout1): Dropout(p=0.1, inplace=False)\n",
       "      (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      (layer_norm1): CNNLayerNorm(\n",
       "        (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (layer_norm2): CNNLayerNorm(\n",
       "        (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (2): ResidualCNN(\n",
       "      (cnn1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (cnn2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (dropout1): Dropout(p=0.1, inplace=False)\n",
       "      (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      (layer_norm1): CNNLayerNorm(\n",
       "        (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (layer_norm2): CNNLayerNorm(\n",
       "        (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (fully_connected): Linear(in_features=2048, out_features=512, bias=True)\n",
       "  (birnn_layers): Sequential(\n",
       "    (0): BidirectionalGRU(\n",
       "      (BiGRU): GRU(512, 512, batch_first=True, bidirectional=True)\n",
       "      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): BidirectionalGRU(\n",
       "      (BiGRU): GRU(1024, 512, bidirectional=True)\n",
       "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): BidirectionalGRU(\n",
       "      (BiGRU): GRU(1024, 512, bidirectional=True)\n",
       "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): BidirectionalGRU(\n",
       "      (BiGRU): GRU(1024, 512, bidirectional=True)\n",
       "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): BidirectionalGRU(\n",
       "      (BiGRU): GRU(1024, 512, bidirectional=True)\n",
       "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=1024, out_features=512, bias=True)\n",
       "    (1): GELU()\n",
       "    (2): Dropout(p=0.1, inplace=False)\n",
       "    (3): Linear(in_features=512, out_features=29, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = SpeechRecognitionModel(\n",
    "    hyperparameters['n_cnn_layers'], hyperparameters['n_rnn_layers'], \n",
    "    hyperparameters['rnn_dim'], hyperparameters['n_class'], hyperparameters['n_feats'], \n",
    "    hyperparameters['stride'], hyperparameters['dropout']\n",
    "    )\n",
    "print('Number model parameters', sum([param.nelement() for param in model.parameters()]))\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "checkpoint = {'hyperparameters':hyperparameters}\n",
    "checkpoints = main(train_loader, test_loader, hyperparameters, model, checkpoint=checkpoint)\n",
    "torch.save(checkpoints[-1], 'last_checkpoint.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "tensor([13, 19, 16, 18, 17, 25, 20, 13, 17, 20, 12, 21, 17, 21, 20, 10, 17, 12,\n",
      "         5, 14, 17, 20, 13,  2, 13, 18, 13, 17, 19, 13, 10], dtype=torch.int32)\n",
      "[13 19 16 18 17 25 20 13 17 20 12 21 17 21 20 10 17 12  5 14 17 20 13  2\n",
      " 13 18 13 17 19 13 10]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'lroqpxslpsktptsipkdmpslalqlprli'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder = CTCBeamDecoder(\n",
    "    char_map_str,\n",
    "    alpha=0.5,\n",
    "    beta=0,\n",
    "    cutoff_top_n=40,\n",
    "    cutoff_prob=1.0,\n",
    "    beam_width=100,\n",
    "    num_processes=4,\n",
    "    blank_id=char_map_str.index('_'),\n",
    "    log_probs_input=False\n",
    ")\n",
    "predict(model, '121-123852-0001.flac', decoder)  # audio file without transformations"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
